"""
refiner_agent.py
MedGemma Multi-AI Agentic System

Author: Aaron Masuba
License: MIT
"""

from __future__ import annotations

import asyncio
import re
import time
from typing import Any, Dict, List, Tuple

from loguru import logger

from .base_agent import (
    BaseAgent,
    AgentNotInitializedError,
    AgentTimeoutError,
)


class RefinerAgent(BaseAgent):
    """
    Extracts and structures key findings from a draft report, vision analysis,
    and retrieved context.

    Payload contract
    ----------------
    • draft_report      : str  — text generated by DraftAgent
    • vision_analysis   : str  — visual observations from VisionAgent
    • retrieved_context : Dict — output of RetrievalAgent (optional)
    • return_mode       : str  — "structured" | "summary" | "both"   (default: both)
    """

    DEFAULT_SECTIONS = ("FINDINGS", "IMPRESSION", "RECOMMENDATIONS")

    def __init__(self, cfg: Dict[str, Any], model_wrapper: "MedGemmaWrapper"):
        super().__init__(cfg, global_cfg=cfg.get("global", {}))
        self.model = model_wrapper

        # Config
        self.entity_extraction = cfg["config"].get("entity_extraction", True)
        self.finding_prioritization = cfg["config"].get("finding_prioritization", True)
        self.consistency_checking = cfg["config"].get("consistency_checking", True)
        self.output_format = cfg["config"].get("output_format", "structured")

        # Regex patterns (can be replaced by more sophisticated NLP later)
        self._disease_re = re.compile(
            r"\b(pneumonia|effusion|atelectasis|nodule|opacity|infiltrate|"
            r"edema|fracture|consolidation|cardiomegaly|emphysema|pneumothorax)\b",
            flags=re.I,
        )
        self._location_re = re.compile(
            r"\b(right|left|upper|middle|lower|bilateral|basal|apical|"
            r"perihilar|subpleural)\b",
            flags=re.I,
        )
        self._severity_re = re.compile(
            r"\b(mild|moderate|severe|small|large|minimal|marked)\b",
            flags=re.I,
        )
        self._uncertainty_re = re.compile(r"\b(probable|possible|suggests?)\b", flags=re.I)

    # ------------------------------------------------------------------ #
    # Lifecycle
    # ------------------------------------------------------------------ #

    async def _initialize(self) -> None:
        if not self.model or not self.model.initialized:
            raise AgentNotInitializedError(
                "MedGemmaWrapper must be initialized before RefinerAgent"
            )
        self.log.info("RefinerAgent ready ✔")

    async def _shutdown(self) -> None:
        # nothing to clean up
        self.log.debug("RefinerAgent shutdown complete")

    # ------------------------------------------------------------------ #
    # Core logic
    # ------------------------------------------------------------------ #

    async def _process(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        draft_report = payload.get("draft_report") or ""
        vision_analysis = payload.get("vision_analysis") or ""
        retrieved_context = payload.get("retrieved_context", {})
        return_mode = payload.get("return_mode", "both").lower()

        if not draft_report:
            raise ValueError("Payload must include 'draft_report'")

        t0 = time.perf_counter()

        # 1. Aggregate context for consistency checking
        context_str = self._build_context_string(vision_analysis, retrieved_context)

        # 2. Build prompt for LLM-based extraction (if enabled)
        findings_structured: List[Dict[str, str]] = []
        if self.entity_extraction:
            prompt = self._build_extraction_prompt(draft_report, context_str)
            try:
                llm_out = await asyncio.wait_for(
                    self.model.generate(
                        prompt=prompt,
                        max_length=512,
                        temperature=0.0,
                        do_sample=False,
                    ),
                    timeout=self._timeout,
                )
                findings_structured = self._parse_llm_json(llm_out)
            except asyncio.TimeoutError as exc:
                self.log.error("RefinerAgent timed-out during entity extraction")
                raise AgentTimeoutError from exc
            except Exception as exc:  # noqa: BLE001
                self.log.warning(f"LLM extraction failed: {exc}")

        # 3. Fallback / enrichment with regex heuristics
        if not findings_structured:
            findings_structured = self._regex_extract(draft_report)

        # 4. Prioritization
        if self.finding_prioritization:
            findings_structured = self._prioritize_findings(findings_structured)

        elapsed = time.perf_counter() - t0
        self.log.info(f"Refiner completed in {elapsed:.2f}s")

        result: Dict[str, Any] = {
            "structured_findings": findings_structured,
            "num_findings": len(findings_structured),
            "timestamp": time.time(),
            "processing_time": elapsed,
        }

        if return_mode in {"summary", "both"}:
            result["findings_summary"] = self._to_summary(findings_structured)

        return result

    # ------------------------------------------------------------------ #
    # Helper – context builder
    # ------------------------------------------------------------------ #

    def _build_context_string(self, vision_analysis: str, retrieved_context: Dict) -> str:
        parts: List[str] = []
        if vision_analysis:
            parts.append(f"Vision Analysis:\n{vision_analysis}")
        if retrieved_context and retrieved_context.get("matched_reports"):
            parts.append("Similar Reports:")
            for r in retrieved_context["matched_reports"][:2]:
                parts.append(f"- {r.get('content', '')[:250]} ...")
        return "\n\n".join(parts)

    # ------------------------------------------------------------------ #
    # Helper – extraction prompt
    # ------------------------------------------------------------------ #

    def _build_extraction_prompt(self, draft: str, ctx: str) -> str:
        return (
            "You are an expert medical NLP system. Extract all distinct findings from the "
            "following chest-X-ray report. For each finding, output JSON objects with keys:\n"
            "  • 'disease'       – radiological abnormality\n"
            "  • 'location'      – anatomic location(s), if given\n"
            "  • 'severity'      – severity descriptor, if present\n"
            "  • 'uncertainty'   – true/false if finding expressed as uncertain\n"
            "Return **ONLY** a JSON array.\n\n"
            f"Report:\n{draft}\n\n"
            f"{ctx if ctx else ''}"
        )

    # ------------------------------------------------------------------ #
    # Helper – parse JSON output
    # ------------------------------------------------------------------ #

    def _parse_llm_json(self, text: str) -> List[Dict[str, str]]:
        import json, re  # noqa: E402

        # find first '[' ... ']'
        m = re.search(r"\[.*\]", text, re.S)
        if not m:
            raise ValueError("No JSON array found in LLM output")
        try:
            data = json.loads(m.group(0))
            if isinstance(data, list):
                return data  # type: ignore[return-value]
            raise ValueError("JSON is not a list")
        except Exception as exc:  # noqa: BLE001
            raise ValueError("Failed to parse JSON from LLM") from exc

    # ------------------------------------------------------------------ #
    # Helper – regex extraction (fallback & enrichment)
    # ------------------------------------------------------------------ #

    def _regex_extract(self, text: str) -> List[Dict[str, str]]:
        findings: List[Dict[str, str]] = []
        sentences = re.split(r"[.;]\s*", text)
        for sent in sentences:
            disease = self._first_match(self._disease_re, sent)
            if not disease:
                continue
            finding: Dict[str, str] = {"disease": disease.lower()}
            location = self._first_match(self._location_re, sent)
            if location:
                finding["location"] = location.lower()
            severity = self._first_match(self._severity_re, sent)
            if severity:
                finding["severity"] = severity.lower()
            if self._uncertainty_re.search(sent):
                finding["uncertainty"] = "true"
            findings.append(finding)
        return findings

    @staticmethod
    def _first_match(pattern: re.Pattern, text: str) -> str:
        m = pattern.search(text)
        return m.group(0) if m else ""

    # ------------------------------------------------------------------ #
    # Helper – prioritization
    # ------------------------------------------------------------------ #

    def _prioritize_findings(self, findings: List[Dict[str, str]]) -> List[Dict[str, str]]:
        # simple heuristic: severe > moderate > mild, else original order
        severity_rank = {"severe": 0, "large": 0, "marked": 0,
                         "moderate": 1, "small": 2, "mild": 2}
        return sorted(
            findings,
            key=lambda f: severity_rank.get(f.get("severity", ""), 3)
        )

    # ------------------------------------------------------------------ #
    # Helper – summary generator
    # ------------------------------------------------------------------ #

    def _to_summary(self, findings: List[Dict[str, str]]) -> str:
        if not findings:
            return "No abnormal findings detected."
        parts = []
        for f in findings:
            disease = f["disease"]
            loc = f.get("location", "")
            sev = f.get("severity", "")
            unc = "uncertain " if f.get("uncertainty") == "true" else ""
            seg = " ".join(x for x in [sev, loc, disease] if x).strip()
            parts.append(f"- {unc}{seg}")
        return "Key Findings:\n" + "\n".join(parts)


# --------------------------------------------------------------------------- #
# Stand-alone quick test
# --------------------------------------------------------------------------- #

if __name__ == "__main__":  # pragma: no cover
    import argparse
    import json
    from medgemma_multiagent.models.medgemma_wrapper import MedGemmaWrapper

    parser = argparse.ArgumentParser(description="RefinerAgent demo")
    parser.add_argument("--draft_path", required=True, help="Path to text file w/ draft")
    parser.add_argument("--cfg", default="config.yaml")
    args = parser.parse_args()

    async def _demo() -> None:
        model = MedGemmaWrapper(config_path=args.cfg)
        await model.initialize()

        agent_cfg: Dict[str, Any] = {
            "name": "RefinerAgent",
            "description": "Standalone Refiner agent demo",
            "config": {},  # defaults
        }
        agent = RefinerAgent(agent_cfg, model)
        await agent.initialize()

        with open(args.draft_path, "r", encoding="utf-8") as f:
            draft_text = f.read()

        result = await agent.process({"draft_report": draft_text})
        print(json.dumps(result, indent=2))

        await agent.shutdown()
        await model.cleanup()

    asyncio.run(_demo())
